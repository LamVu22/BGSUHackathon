#!/usr/bin/env python3
"""Build graph metrics from cleaned node/link data.

Expects data/processed/clean_nodes.json and clean_edges.json (generated by
scripts/clean_content.py). Loads those files, creates a directed graph, and
computes metrics such as PageRank, betweenness, degree counts, hop distances,
and depth from the homepage.

Usage (run from repo root with venv activated):

    python scripts/build_graph.py

Configuration lives in config/pipeline.json (or override with PIPELINE_CONFIG).
"""

from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List
from urllib.parse import urlparse

import networkx as nx

REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_CONFIG_PATH = REPO_ROOT / "config" / "pipeline.json"


@dataclass
class GraphSettings:
    processed_output: Path = REPO_ROOT / "data/processed"
    root_url: str = "https://www.bgsu.edu"

    @classmethod
    def from_dict(cls, data: Dict[str, object]) -> "GraphSettings":
        default = cls()

        def get_value(key: str, fallback):
            return data.get(key, fallback) if isinstance(data, dict) else fallback

        processed_output = _resolve_path(get_value("processed_output", default.processed_output))

        return cls(
            processed_output=processed_output,
            root_url=get_value("root_url", default.root_url),
        )

    @property
    def clean_nodes_path(self) -> Path:
        return self.processed_output / "clean_nodes.json"

    @property
    def clean_edges_path(self) -> Path:
        return self.processed_output / "clean_edges.json"

    @property
    def nodes_output_path(self) -> Path:
        return self.processed_output / "nodes.json"

    @property
    def edges_output_path(self) -> Path:
        return self.processed_output / "edges.json"


class GraphBuilder:
    def __init__(self, settings: GraphSettings) -> None:
        self.settings = settings
        self.nodes: Dict[str, Dict] = {}
        self.edges: List[Dict] = []
        self.graph = nx.DiGraph()

    def build(self) -> None:
        if not self._load_clean_data():
            return
        self._build_graph()
        self._compute_metrics()
        self._write_outputs()

    def _load_clean_data(self) -> bool:
        if not self.settings.clean_nodes_path.exists() or not self.settings.clean_edges_path.exists():
            logging.error(
                "Missing cleaned data. Expected %s and %s",
                self.settings.clean_nodes_path,
                self.settings.clean_edges_path,
            )
            logging.error("Run scripts/clean_content.py first.")
            return False
        with self.settings.clean_nodes_path.open("r", encoding="utf-8") as f:
            nodes_list = json.load(f)
        with self.settings.clean_edges_path.open("r", encoding="utf-8") as f:
            self.edges = json.load(f)

        for node in nodes_list:
            self.nodes[node["url"].rstrip("/")] = node
        logging.info("Loaded %s nodes and %s edges from cleaned data", len(self.nodes), len(self.edges))
        return True

    def _build_graph(self) -> None:
        for url, data in self.nodes.items():
            self.graph.add_node(url, **data)

        for edge in self.edges:
            source = edge.get("source", "").rstrip("/")
            target = edge.get("target", "").rstrip("/")
            if not source or not target:
                continue
            if target not in self.nodes:
                self.nodes[target] = {
                    "url": target,
                    "path": "",
                    "content_type": "",
                    "doc_type": "external",
                    "title": None,
                    "word_count": 0,
                    "clean_text": "",
                    "snippet": "",
                    "domain": urlparse(target).netloc,
                    "is_root": False,
                }
                self.graph.add_node(target, **self.nodes[target])
            self.graph.add_edge(source, target, **{k: v for k, v in edge.items() if k not in {"source", "target"}})

    def _compute_metrics(self) -> None:
        if self.graph.number_of_nodes() == 0:
            return

        pagerank = nx.pagerank(self.graph, alpha=0.85, max_iter=200)
        betweenness = nx.betweenness_centrality(self.graph, normalized=True)
        depth = {}
        root = self.settings.root_url.rstrip("/")
        if root in self.graph:
            depth = nx.single_source_shortest_path_length(self.graph, root)

        parents_map: Dict[str, set] = {node: set() for node in self.graph.nodes}
        children_map: Dict[str, set] = {node: set() for node in self.graph.nodes}
        internal_out = {node: 0 for node in self.graph.nodes}
        external_out = {node: 0 for node in self.graph.nodes}

        for source, target in self.graph.edges:
            parents_map[target].add(source)
            children_map[source].add(target)
            source_domain = urlparse(source).netloc
            target_domain = urlparse(target).netloc
            if source_domain == target_domain:
                internal_out[source] += 1
            else:
                external_out[source] += 1

        for edge in self.edges:
            source = edge.get("source", "").rstrip("/")
            target = edge.get("target", "").rstrip("/")
            source_depth = depth.get(source)
            target_depth = depth.get(target)
            edge["source_depth"] = source_depth
            edge["target_depth"] = target_depth
            if source_depth is not None and target_depth is not None:
                edge["hop_distance"] = target_depth - source_depth
            else:
                edge["hop_distance"] = None

        for url, data in self.nodes.items():
            data.setdefault("metrics", {})
            data["metrics"].update(
                {
                    "in_degree": self.graph.in_degree(url),
                    "out_degree": self.graph.out_degree(url),
                    "pagerank": pagerank.get(url, 0.0),
                    "betweenness": betweenness.get(url, 0.0),
                    "depth_from_root": depth.get(url),
                    "internal_outgoing": internal_out.get(url, 0),
                    "external_outgoing": external_out.get(url, 0),
                }
            )
            data["parents"] = sorted(parents_map.get(url, []))[:20]
            data["children"] = sorted(children_map.get(url, []))[:20]

    def _write_outputs(self) -> None:
        self.settings.processed_output.mkdir(parents=True, exist_ok=True)
        with self.settings.nodes_output_path.open("w", encoding="utf-8") as f:
            json.dump(list(self.nodes.values()), f, indent=2)
        with self.settings.edges_output_path.open("w", encoding="utf-8") as f:
            json.dump(self.edges, f, indent=2)
        logging.info(
            "Wrote enriched graph to %s and %s",
            self.settings.nodes_output_path,
            self.settings.edges_output_path,
        )


def _resolve_path(path_value) -> Path:
    path = path_value if isinstance(path_value, Path) else Path(path_value)
    if not path.is_absolute():
        path = REPO_ROOT / path
    return path


def load_settings(config_path: Path | None = None) -> GraphSettings:
    path = config_path or DEFAULT_CONFIG_PATH
    if path.exists():
        try:
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            logging.info("Loaded pipeline config from %s", path)
            return GraphSettings.from_dict(data)
        except json.JSONDecodeError as exc:
            logging.error("Failed to parse config %s: %s", path, exc)
    else:
        logging.warning("Config file %s not found. Using defaults.", path)
    return GraphSettings()


def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    config_path = Path(os.environ["PIPELINE_CONFIG"]) if "PIPELINE_CONFIG" in os.environ else None
    settings = load_settings(config_path)
    builder = GraphBuilder(settings)
    builder.build()


if __name__ == "__main__":
    main()
